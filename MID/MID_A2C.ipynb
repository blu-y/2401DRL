{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(nn.Module):\n",
    "    \"\"\"\n",
    "    (Synchronous) Advantage Actor-Critic agent class\n",
    "\n",
    "    Args:\n",
    "        n_features: The number of features of the input state.\n",
    "        n_actions: The number of actions the agent can take.\n",
    "        device: The device to run the computations on (running on a GPU might be quicker for larger Neural Nets,\n",
    "                for this code CPU is totally fine).\n",
    "        critic_lr: The learning rate for the critic network (should usually be larger than the actor_lr).\n",
    "        actor_lr: The learning rate for the actor network.\n",
    "        n_envs: The number of environments that run in parallel (on multiple CPUs) to collect experiences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        n_actions: int,\n",
    "        device: torch.device,\n",
    "        critic_lr: float,\n",
    "        actor_lr: float,\n",
    "        n_envs: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes the actor and critic networks and their respective optimizers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_envs = n_envs\n",
    "\n",
    "        critic_layers = [\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),  # estimate V(s)\n",
    "        ]\n",
    "\n",
    "        actor_layers = [\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                32, n_actions\n",
    "            ),  # estimate action logits (will be fed into a softmax later)\n",
    "        ]\n",
    "\n",
    "        # define actor and critic networks\n",
    "        self.critic = nn.Sequential(*critic_layers).to(self.device)\n",
    "        self.actor = nn.Sequential(*actor_layers).to(self.device)\n",
    "\n",
    "        # define optimizers for actor and critic\n",
    "        self.critic_optim = optim.RMSprop(self.critic.parameters(), lr=critic_lr)\n",
    "        self.actor_optim = optim.RMSprop(self.actor.parameters(), lr=actor_lr)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the networks.\n",
    "\n",
    "        Args:\n",
    "            x: A batched vector of states.\n",
    "\n",
    "        Returns:\n",
    "            state_values: A tensor with the state values, with shape [n_envs,].\n",
    "            action_logits_vec: A tensor with the action logits, with shape [n_envs, n_actions].\n",
    "        \"\"\"\n",
    "        x = torch.Tensor(x).to(self.device)\n",
    "        state_values = self.critic(x)  # shape: [n_envs,]\n",
    "        action_logits_vec = self.actor(x)  # shape: [n_envs, n_actions]\n",
    "        return (state_values, action_logits_vec)\n",
    "\n",
    "    def select_action(\n",
    "        self, x: np.ndarray\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns a tuple of the chosen actions and the log-probs of those actions.\n",
    "\n",
    "        Args:\n",
    "            x: A batched vector of states.\n",
    "\n",
    "        Returns:\n",
    "            actions: A tensor with the actions, with shape [n_steps_per_update, n_envs].\n",
    "            action_log_probs: A tensor with the log-probs of the actions, with shape [n_steps_per_update, n_envs].\n",
    "            state_values: A tensor with the state values, with shape [n_steps_per_update, n_envs].\n",
    "        \"\"\"\n",
    "        state_values, action_logits = self.forward(x)\n",
    "        action_pd = torch.distributions.Categorical(\n",
    "            logits=action_logits\n",
    "        )  # implicitly uses softmax\n",
    "        actions = action_pd.sample()\n",
    "        action_log_probs = action_pd.log_prob(actions)\n",
    "        entropy = action_pd.entropy()\n",
    "        return (actions, action_log_probs, state_values, entropy)\n",
    "\n",
    "    def get_losses(\n",
    "        self,\n",
    "        rewards: torch.Tensor,\n",
    "        action_log_probs: torch.Tensor,\n",
    "        value_preds: torch.Tensor,\n",
    "        entropy: torch.Tensor,\n",
    "        masks: torch.Tensor,\n",
    "        gamma: float,\n",
    "        lam: float,\n",
    "        ent_coef: float,\n",
    "        device: torch.device,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the loss of a minibatch (transitions collected in one sampling phase) for actor and critic\n",
    "        using Generalized Advantage Estimation (GAE) to compute the advantages (https://arxiv.org/abs/1506.02438).\n",
    "\n",
    "        Args:\n",
    "            rewards: A tensor with the rewards for each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            action_log_probs: A tensor with the log-probs of the actions taken at each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            value_preds: A tensor with the state value predictions for each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            masks: A tensor with the masks for each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            gamma: The discount factor.\n",
    "            lam: The GAE hyperparameter. (lam=1 corresponds to Monte-Carlo sampling with high variance and no bias,\n",
    "                                          and lam=0 corresponds to normal TD-Learning that has a low variance but is biased\n",
    "                                          because the estimates are generated by a Neural Net).\n",
    "            device: The device to run the computations on (e.g. CPU or GPU).\n",
    "\n",
    "        Returns:\n",
    "            critic_loss: The critic loss for the minibatch.\n",
    "            actor_loss: The actor loss for the minibatch.\n",
    "        \"\"\"\n",
    "        T = len(rewards)\n",
    "        advantages = torch.zeros(T, self.n_envs, device=device)\n",
    "\n",
    "        # compute the advantages using GAE\n",
    "        gae = 0.0\n",
    "        for t in reversed(range(T - 1)):\n",
    "            td_error = (\n",
    "                rewards[t] + gamma * masks[t] * value_preds[t + 1] - value_preds[t]\n",
    "            )\n",
    "            gae = td_error + gamma * lam * masks[t] * gae\n",
    "            advantages[t] = gae\n",
    "\n",
    "        # calculate the loss of the minibatch for actor and critic\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "\n",
    "        # give a bonus for higher entropy to encourage exploration\n",
    "        actor_loss = (\n",
    "            -(advantages.detach() * action_log_probs).mean() - ent_coef * entropy.mean()\n",
    "        )\n",
    "        return (critic_loss, actor_loss)\n",
    "\n",
    "    def update_parameters(\n",
    "        self, critic_loss: torch.Tensor, actor_loss: torch.Tensor\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Updates the parameters of the actor and critic networks.\n",
    "\n",
    "        Args:\n",
    "            critic_loss: The critic loss.\n",
    "            actor_loss: The actor loss.\n",
    "        \"\"\"\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 38\u001b[0m\n\u001b[1;32m     35\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# init the agent\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mA2C\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_lr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# create a wrapper environment to save episode returns and episode lengths\u001b[39;00m\n\u001b[1;32m     40\u001b[0m envs_wrapper \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mwrappers\u001b[38;5;241m.\u001b[39mRecordEpisodeStatistics(envs, deque_size\u001b[38;5;241m=\u001b[39mn_envs \u001b[38;5;241m*\u001b[39m n_updates)\n",
      "Cell \u001b[0;32mIn[15], line 48\u001b[0m, in \u001b[0;36mA2C.__init__\u001b[0;34m(self, n_features, n_actions, device, critic_lr, actor_lr, n_envs)\u001b[0m\n\u001b[1;32m     37\u001b[0m actor_layers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     38\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear(n_features, \u001b[38;5;241m32\u001b[39m),\n\u001b[1;32m     39\u001b[0m     nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     ),  \u001b[38;5;66;03m# estimate action logits (will be fed into a softmax later)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m ]\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# define actor and critic networks\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSequential\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcritic_layers\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39mactor_layers)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# define optimizers for actor and critic\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco/lib/python3.10/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# environment hyperparams\n",
    "n_envs = 10\n",
    "n_updates = 1000\n",
    "n_steps_per_update = 128\n",
    "randomize_domain = True\n",
    "# agent hyperparams\n",
    "gamma = 0.999\n",
    "lam = 0.95  # hyperparameter for GAE\n",
    "ent_coef = 0.01  # coefficient for the entropy bonus (to encourage exploration)\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.005\n",
    "envs = gym.vector.AsyncVectorEnv(\n",
    "    [\n",
    "        lambda: gym.make(\n",
    "            \"Swimmer-v4\",\n",
    "            # xml_file=\"swimmer.xml\",\n",
    "            forward_reward_weight=1.0,\n",
    "            ctrl_cost_weight=1e-4,\n",
    "            reset_noise_scale=0.1,\n",
    "            exclude_current_positions_from_observation=True,\n",
    "            max_episode_steps=600,\n",
    "        )\n",
    "        for i in range(n_envs)\n",
    "    ]\n",
    ")\n",
    "\n",
    "obs_shape = envs.single_observation_space.shape[0]\n",
    "action_shape = envs.single_action_space.shape[0]\n",
    "\n",
    "# set the device\n",
    "use_cuda = True\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# init the agent\n",
    "agent = A2C(obs_shape, action_shape, device, critic_lr, actor_lr, n_envs)\n",
    "# create a wrapper environment to save episode returns and episode lengths\n",
    "envs_wrapper = gym.wrappers.RecordEpisodeStatistics(envs, deque_size=n_envs * n_updates)\n",
    "\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "entropies = []\n",
    "\n",
    "# use tqdm to get a progress bar for training\n",
    "for sample_phase in tqdm(range(n_updates)):\n",
    "    # we don't have to reset the envs, they just continue playing\n",
    "    # until the episode is over and then reset automatically\n",
    "\n",
    "    # reset lists that collect experiences of an episode (sample phase)\n",
    "    ep_value_preds = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_rewards = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_action_log_probs = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    masks = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "\n",
    "    # at the start of training reset all envs to get an initial state\n",
    "    if sample_phase == 0:\n",
    "        states, info = envs_wrapper.reset(seed=42)\n",
    "\n",
    "    # play n steps in our parallel environments to collect data\n",
    "    for step in range(n_steps_per_update):\n",
    "        # select an action A_{t} using S_{t} as input for the agent\n",
    "        actions, action_log_probs, state_value_preds, entropy = agent.select_action(\n",
    "            states\n",
    "        )\n",
    "\n",
    "        # perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}\n",
    "        states, rewards, terminated, truncated, infos = envs_wrapper.step(\n",
    "            actions.cpu().numpy()\n",
    "        )\n",
    "\n",
    "        ep_value_preds[step] = torch.squeeze(state_value_preds)\n",
    "        ep_rewards[step] = torch.tensor(rewards, device=device)\n",
    "        ep_action_log_probs[step] = action_log_probs\n",
    "\n",
    "        # add a mask (for the return calculation later);\n",
    "        # for each env the mask is 1 if the episode is ongoing and 0 if it is terminated (not by truncation!)\n",
    "        masks[step] = torch.tensor([not term for term in terminated])\n",
    "\n",
    "    # calculate the losses for actor and critic\n",
    "    critic_loss, actor_loss = agent.get_losses(\n",
    "        ep_rewards,\n",
    "        ep_action_log_probs,\n",
    "        ep_value_preds,\n",
    "        entropy,\n",
    "        masks,\n",
    "        gamma,\n",
    "        lam,\n",
    "        ent_coef,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    # update the actor and critic networks\n",
    "    agent.update_parameters(critic_loss, actor_loss)\n",
    "\n",
    "    # log the losses and entropy\n",
    "    critic_losses.append(critic_loss.detach().cpu().numpy())\n",
    "    actor_losses.append(actor_loss.detach().cpu().numpy())\n",
    "    entropies.append(entropy.detach().mean().cpu().numpy())\n",
    "\n",
    "\"\"\" plot the results \"\"\"\n",
    "\n",
    "# %matplotlib inline\n",
    "\n",
    "rolling_length = 20\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "fig.suptitle(\n",
    "    f\"Training plots for {agent.__class__.__name__} in the Swimmer-v4 environment \\n \\\n",
    "            (n_envs={n_envs}, n_steps_per_update={n_steps_per_update}, randomize_domain={randomize_domain})\"\n",
    ")\n",
    "\n",
    "# episode return\n",
    "axs[0][0].set_title(\"Episode Returns\")\n",
    "episode_returns_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(envs_wrapper.return_queue).flatten(),\n",
    "        np.ones(rolling_length),\n",
    "        mode=\"valid\",\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][0].plot(\n",
    "    np.arange(len(episode_returns_moving_average)) / n_envs,\n",
    "    episode_returns_moving_average,\n",
    ")\n",
    "axs[0][0].set_xlabel(\"Number of episodes\")\n",
    "\n",
    "# entropy\n",
    "axs[1][0].set_title(\"Entropy\")\n",
    "entropy_moving_average = (\n",
    "    np.convolve(np.array(entropies), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][0].plot(entropy_moving_average)\n",
    "axs[1][0].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# critic loss\n",
    "axs[0][1].set_title(\"Critic Loss\")\n",
    "critic_losses_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(critic_losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][1].plot(critic_losses_moving_average)\n",
    "axs[0][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# actor loss\n",
    "axs[1][1].set_title(\"Actor Loss\")\n",
    "actor_losses_moving_average = (\n",
    "    np.convolve(np.array(actor_losses).flatten(), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][1].plot(actor_losses_moving_average)\n",
    "axs[1][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_weights = True\n",
    "\n",
    "actor_weights_path = \"weights/actor_weights.h5\"\n",
    "critic_weights_path = \"weights/critic_weights.h5\"\n",
    "\n",
    "if not os.path.exists(\"weights\"):\n",
    "    os.mkdir(\"weights\")\n",
    "\n",
    "\"\"\" save network weights \"\"\"\n",
    "if save_weights:\n",
    "    torch.save(agent.actor.state_dict(), actor_weights_path)\n",
    "    torch.save(agent.critic.state_dict(), critic_weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_weights = False\n",
    "\n",
    "\"\"\" load network weights \"\"\"\n",
    "if load_weights:\n",
    "    agent = A2C(obs_shape, action_shape, device, critic_lr, actor_lr, n_envs)\n",
    "\n",
    "    agent.actor.load_state_dict(torch.load(actor_weights_path))\n",
    "    agent.critic.load_state_dict(torch.load(critic_weights_path))\n",
    "    agent.actor.eval()\n",
    "    agent.critic.eval()\n",
    "\n",
    "\"\"\" play a couple of showcase episodes \"\"\"\n",
    "\n",
    "n_showcase_episodes = 5\n",
    "\n",
    "for episode in range(n_showcase_episodes):\n",
    "    print(f\"starting episode {episode}...\")\n",
    "\n",
    "    # create a new sample environment to get new random parameters\n",
    "    if randomize_domain:\n",
    "        env = gym.make(\n",
    "            \"LunarLander-v2\",\n",
    "            render_mode=\"human\",\n",
    "            gravity=np.clip(\n",
    "                np.random.normal(loc=-10.0, scale=2.0), a_min=-11.99, a_max=-0.01\n",
    "            ),\n",
    "            enable_wind=np.random.choice([True, False]),\n",
    "            wind_power=np.clip(\n",
    "                np.random.normal(loc=15.0, scale=2.0), a_min=0.01, a_max=19.99\n",
    "            ),\n",
    "            turbulence_power=np.clip(\n",
    "                np.random.normal(loc=1.5, scale=1.0), a_min=0.01, a_max=1.99\n",
    "            ),\n",
    "            max_episode_steps=500,\n",
    "        )\n",
    "    else:\n",
    "        env = gym.make(\"LunarLander-v2\", render_mode=\"human\", max_episode_steps=500)\n",
    "\n",
    "    # get an initial state\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # play one episode\n",
    "    done = False\n",
    "    while not done:\n",
    "        # select an action A_{t} using S_{t} as input for the agent\n",
    "        with torch.no_grad():\n",
    "            action, _, _, _ = agent.select_action(state[None, :])\n",
    "\n",
    "        # perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}\n",
    "        state, reward, terminated, truncated, info = env.step(action.item())\n",
    "\n",
    "        # update if the environment is done\n",
    "        done = terminated or truncated\n",
    "        if done: print(reward)\n",
    "\n",
    "env.close()\n",
    "\n",
    "# from gymnasium.utils.play import play\n",
    "#\n",
    "# play(gym.make('LunarLander-v2', render_mode='rgb_array'),\n",
    "#     keys_to_action={'w': 2, 'a': 1, 'd': 3}, noop=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
