{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1*. Implement A2C using gym mujoco Swimmer environment.\n",
    "# 2. Implement domain randomization in Swimmer environment.\n",
    "# 3*. Use normal distribution for continuous action space.\n",
    "# 4. Implement n-step return TD error without lambda GAE.\n",
    "# 5. Use transformer network.\n",
    "# 6. Excellent episode returns.\n",
    "# 7*. Demonstrate the agent with best weights.\n",
    "\n",
    "### gymnasium = 0.29.1\n",
    "### mujoco = 2.3.7\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.distributions as D\n",
    "from tqdm import tqdm\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Space: Box(-1 ~ 1, (6,), float32)\n",
    "0: Torque applied on the thigh rotor, \"thigh_joint\"  \n",
    "1: Torque applied on the leg rotor, \"leg_joint\"  \n",
    "2: Torque applied on the foot rotor, \"foot_joint\"  \n",
    "3: Torque applied on the left thigh rotor, \"thigh_left_joint\"  \n",
    "4: Torque applied on the left leg rotor, \"leg_left_joint\"  \n",
    "5: Torque applied on the left foot rotor, \"foot_left_joint\"  \n",
    "\n",
    "#### Observation Space: Box(-Inf, Inf, (17,), float64)\n",
    "X: x-coordinate of the torso, \"rootx\", slide, position(m)  \n",
    "0: z-coordinate of the torso, \"rootz\", slide, position(m)  \n",
    "1: angle of the torso, \"rooty\", hinge, angle(rad)  \n",
    "2: angle of the thigh joint, \"thigh_joint\", hinge, angle(rad)  \n",
    "3: angle of the leg joint, \"leg_joint\", hinge, angle(rad)  \n",
    "4: angle of the foot joint, \"foot_joint\", hinge, angle(rad)  \n",
    "5: angle of the left thigh joint, \"thigh_left_joint\", hinge, angle(rad)  \n",
    "6: angle of the left leg joint, \"leg_left_joint\", hinge, angle(rad)  \n",
    "7: angle of the left foot joint, \"foot_left_joint\", hinge, angle(rad)  \n",
    "8: velocity of the x-coordinate of the torso, \"rootx\", slide, velocity(m/s)  \n",
    "9: velocity of the z-coordinate(height) of the torso, \"rootz\", slide, velocity(m/s)  \n",
    "10: angular velocity of the angle of the torso, \"rooty\", hinge, angular velocity(rad/s)  \n",
    "11: angular velocity of the thigh joint, \"thigh_joint\", hinge, angular velocity(rad/s)  \n",
    "12: angular velocity of the leg hinge, \"leg_joint\", hinge, angular velocity(rad/s)  \n",
    "13: angular velocity of the foot hinge, \"foot_joint\", hinge, angular velocity(rad/s)  \n",
    "14: angular velocity of the left thigh hinge, \"thigh_left_joint\", hinge, angular velocity(rad/s)  \n",
    "15: angular velocity of the left leg hinge, \"leg_left_joint\", hinge, angular velocity(rad/s)  \n",
    "16: angular velocity of the left foot hinge, \"foot_left_joint\", hinge, angular velocity(rad/s)  \n",
    "\n",
    "#### Rewards\n",
    "healthy_reward: Every timestep that the walker is alive, it receives a fixed reward of value  \n",
    "forward_reward: This reward would be positive if the walker walks forward (positive x direction).  \n",
    "                forward_reward_weight * (x-coordinate before action - x-coordinate after action)/dt  \n",
    "ctrl_cost: A cost for penalising the walker if it takes actions that are too large.  \n",
    "           ctrl_cost_weight * sum(action^2)  \n",
    "reward = healthy_reward bonus + forward_reward - ctrl_cost  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(nn.Module):\n",
    "    \"\"\"\n",
    "    (Synchronous) Advantage Actor-Critic agent class\n",
    "\n",
    "    Args:\n",
    "        n_features: The number of features of the input state.\n",
    "        n_actions: The number of actions the agent can take.\n",
    "        device: The device to run the computations on (running on a GPU might be quicker for larger Neural Nets,\n",
    "                for this code CPU is totally fine).\n",
    "        critic_lr: The learning rate for the critic network (should usually be larger than the actor_lr).\n",
    "        actor_lr: The learning rate for the actor network.\n",
    "        n_envs: The number of environments that run in parallel (on multiple CPUs) to collect experiences.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_features: int,\n",
    "        n_actions: int,\n",
    "        device: torch.device,\n",
    "        critic_lr: float,\n",
    "        actor_lr: float,\n",
    "        n_envs: int,\n",
    "    ) -> None:\n",
    "        \"\"\"Initializes the actor and critic networks and their respective optimizers.\"\"\"\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_envs = n_envs\n",
    "\n",
    "        critic_layers = [\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),  # estimate V(s)\n",
    "        ]\n",
    "\n",
    "        actor_layers = [\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                32, n_actions\n",
    "            ),  # estimate action logits (will be fed into a softmax later)\n",
    "        ]\n",
    "\n",
    "        # define actor and critic networks\n",
    "        self.critic = nn.Sequential(*critic_layers).to(self.device)\n",
    "        self.actor = nn.Sequential(*actor_layers).to(self.device)\n",
    "\n",
    "        # define optimizers for actor and critic\n",
    "        self.critic_optim = optim.RMSprop(self.critic.parameters(), lr=critic_lr)\n",
    "        self.actor_optim = optim.RMSprop(self.actor.parameters(), lr=actor_lr)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass of the networks.\n",
    "\n",
    "        Args:\n",
    "            x: A batched vector of states.\n",
    "\n",
    "        Returns:\n",
    "            state_values: A tensor with the state values, with shape [n_envs,].\n",
    "            action_logits_vec: A tensor with the action logits, with shape [n_envs, n_actions].\n",
    "        \"\"\"\n",
    "        x = torch.Tensor(x).to(self.device)\n",
    "        state_values = self.critic(x)  # shape: [n_envs,]\n",
    "        action_logits_vec = self.actor(x)  # shape: [n_envs, n_actions]\n",
    "        return (state_values, action_logits_vec)\n",
    "\n",
    "    def select_action(\n",
    "        self, x: np.ndarray\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns a tuple of the chosen actions and the log-probs of those actions.\n",
    "\n",
    "        Args:\n",
    "            x: A batched vector of states.\n",
    "\n",
    "        Returns:\n",
    "            actions: A tensor with the actions, with shape [n_steps_per_update, n_envs].\n",
    "            action_log_probs: A tensor with the log-probs of the actions, with shape [n_steps_per_update, n_envs].\n",
    "            state_values: A tensor with the state values, with shape [n_steps_per_update, n_envs].\n",
    "        \"\"\"\n",
    "        \n",
    "        state_values, action_logits = self.forward(x)\n",
    "        mu = torch.tensor([0.0])\n",
    "        sigma = torch.tensor([1.0])\n",
    "        normal_dist = D.Normal(mu, sigma)# implicitly uses softmax\n",
    "        sampled_value = normal_dist.rsample()\n",
    "        action_log_probs = normal_dist.log_prob(sampled_value)\n",
    "        entropy = normal_dist.entropy()\n",
    "        return (sampled_value, action_log_probs, state_values, entropy)\n",
    "\n",
    "    def get_losses(\n",
    "        self,\n",
    "        rewards: torch.Tensor,\n",
    "        action_log_probs: torch.Tensor,\n",
    "        value_preds: torch.Tensor,\n",
    "        entropy: torch.Tensor,\n",
    "        masks: torch.Tensor,\n",
    "        gamma: float,\n",
    "        lam: float,\n",
    "        ent_coef: float,\n",
    "        device: torch.device,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the loss of a minibatch (transitions collected in one sampling phase) for actor and critic\n",
    "        using Generalized Advantage Estimation (GAE) to compute the advantages (https://arxiv.org/abs/1506.02438).\n",
    "\n",
    "        Args:\n",
    "            rewards: A tensor with the rewards for each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            action_log_probs: A tensor with the log-probs of the actions taken at each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            value_preds: A tensor with the state value predictions for each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            masks: A tensor with the masks for each time step in the episode, with shape [n_steps_per_update, n_envs].\n",
    "            gamma: The discount factor.\n",
    "            lam: The GAE hyperparameter. (lam=1 corresponds to Monte-Carlo sampling with high variance and no bias,\n",
    "                                          and lam=0 corresponds to normal TD-Learning that has a low variance but is biased\n",
    "                                          because the estimates are generated by a Neural Net).\n",
    "            device: The device to run the computations on (e.g. CPU or GPU).\n",
    "\n",
    "        Returns:\n",
    "            critic_loss: The critic loss for the minibatch.\n",
    "            actor_loss: The actor loss for the minibatch.\n",
    "        \"\"\"\n",
    "        T = len(rewards)\n",
    "        advantages = torch.zeros(T, self.n_envs, device=device)\n",
    "\n",
    "        # compute the advantages using GAE\n",
    "        gae = 0.0\n",
    "        for t in reversed(range(T - 1)):\n",
    "            td_error = (\n",
    "                rewards[t] + gamma * masks[t] * value_preds[t + 1] - value_preds[t]\n",
    "            )\n",
    "            gae = td_error + gamma * lam * masks[t] * gae\n",
    "            advantages[t] = gae\n",
    "\n",
    "        # calculate the loss of the minibatch for actor and critic\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "\n",
    "        # give a bonus for higher entropy to encourage exploration\n",
    "        actor_loss = (\n",
    "            -(advantages.detach() * action_log_probs).mean() - ent_coef * entropy.mean()\n",
    "        )\n",
    "        return (critic_loss, actor_loss)\n",
    "\n",
    "    def update_parameters(\n",
    "        self, critic_loss: torch.Tensor, actor_loss: torch.Tensor\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Updates the parameters of the actor and critic networks.\n",
    "\n",
    "        Args:\n",
    "            critic_loss: The critic loss.\n",
    "            actor_loss: The actor loss.\n",
    "        \"\"\"\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousA2C(nn.Module):\n",
    "    def __init__(self, n_features, n_actions, device, critic_lr, actor_lr, n_envs):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_envs = n_envs\n",
    "\n",
    "        # Critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)  # Estimates V(s)\n",
    "        ).to(device)\n",
    "\n",
    "        # Actor network now predicts mean and standard deviation for each action\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(n_features, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_actions),  # Outputs mean of the action distribution\n",
    "            nn.Tanh()  # Optional: use Tanh to bound the actions\n",
    "        ).to(device)\n",
    "\n",
    "        # Additional layer to predict the standard deviation\n",
    "        self.std_dev = nn.Sequential(\n",
    "            nn.Linear(32, n_actions),\n",
    "            nn.Softplus()  # Ensures that the standard deviation is positive\n",
    "        ).to(device)\n",
    "\n",
    "        # Optimizers\n",
    "        self.critic_optim = optim.AdamW(self.critic.parameters(), lr=critic_lr)\n",
    "        self.actor_optim = optim.AdamW(list(self.actor.parameters()) + list(self.std_dev.parameters()), lr=actor_lr)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.Tensor(x).to(self.device)\n",
    "        state_values = self.critic(x)  # shape: [n_envs,]\n",
    "        action_means = self.actor(x)   # shape: [n_envs, n_actions]\n",
    "        action_std_devs = self.std_dev(x)  # shape: [n_envs, n_actions]\n",
    "        return state_values, action_means, action_std_devs\n",
    "\n",
    "    def select_action(self, x):\n",
    "        state_values, action_means, action_std_devs = self.forward(x)\n",
    "        \n",
    "        # Create normal distributions and sample actions using reparameterization trick\n",
    "        dists = D.Normal(action_means, action_std_devs)\n",
    "        actions = dists.rsample()  # Reparameterized sample\n",
    "        log_probs = dists.log_prob(actions).sum(axis=-1)  # Sum log probs for multi-action cases\n",
    "        entropy = dists.entropy().sum(axis=-1)\n",
    "        \n",
    "        return actions, log_probs, state_values, entropy\n",
    "\n",
    "    def get_losses(self, rewards, log_probs, state_values, entropy, masks, gamma, lam, ent_coef):\n",
    "        # Implementation of the loss computation goes here.\n",
    "        # It would be similar to the discrete version but needs to account for the continuous nature of actions.\n",
    "        # Use log_probs and entropy from the new select_action method.\n",
    "        pass\n",
    "\n",
    "    def update_parameters(self, critic_loss, actor_loss):\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_A2C(envs, \n",
    "              n_updates = 1000, \n",
    "              n_steps_per_update = 128, \n",
    "              gamma = 0.999, \n",
    "              lam = 0.95, \n",
    "              ent_coef = 0.01, \n",
    "              actor_lr = 0.001, \n",
    "              critic_lr = 0.005, \n",
    "              n_envs = 10\n",
    "              ):\n",
    "    \n",
    "    obs_shape = envs.single_observation_space.shape[0]\n",
    "    action_shape = envs.single_action_space.shape[0]\n",
    "\n",
    "    # set the device\n",
    "    use_cuda = True\n",
    "    if use_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # init the agent\n",
    "    agent = ContinuousA2C(obs_shape, action_shape, device, critic_lr, actor_lr, n_envs)\n",
    "    # create a wrapper environment to save episode returns and episode lengths\n",
    "    envs_wrapper = gym.wrappers.RecordEpisodeStatistics(envs, deque_size=n_envs * n_updates)\n",
    "\n",
    "    # use tqdm to get a progress bar for training\n",
    "    for sample_phase in tqdm(range(n_updates)):\n",
    "        # we don't have to reset the envs, they just continue playing\n",
    "        # until the episode is over and then reset automatically\n",
    "\n",
    "        # reset lists that collect experiences of an episode (sample phase)\n",
    "        ep_value_preds = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "        ep_rewards = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "        ep_action_log_probs = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "        masks = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "\n",
    "        # at the start of training reset all envs to get an initial state\n",
    "        if sample_phase == 0:\n",
    "            states, info = envs_wrapper.reset(seed=42)\n",
    "\n",
    "        # play n steps in our parallel environments to collect data\n",
    "        for step in range(n_steps_per_update):\n",
    "            # select an action A_{t} using S_{t} as input for the agent\n",
    "            actions, action_log_probs, state_value_preds, entropy = agent.select_action(\n",
    "                states\n",
    "            )\n",
    "\n",
    "            # perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}\n",
    "            states, rewards, terminated, truncated, infos = envs_wrapper.step(\n",
    "                actions.cpu().numpy()\n",
    "            )\n",
    "\n",
    "            ep_value_preds[step] = torch.squeeze(state_value_preds)\n",
    "            ep_rewards[step] = torch.tensor(rewards, device=device)\n",
    "            ep_action_log_probs[step] = action_log_probs\n",
    "\n",
    "            # add a mask (for the return calculation later);\n",
    "            # for each env the mask is 1 if the episode is ongoing and 0 if it is terminated (not by truncation!)\n",
    "            masks[step] = torch.tensor([not term for term in terminated])\n",
    "\n",
    "        # calculate the losses for actor and critic\n",
    "        critic_loss, actor_loss = agent.get_losses(\n",
    "            ep_rewards,\n",
    "            ep_action_log_probs,\n",
    "            ep_value_preds,\n",
    "            entropy,\n",
    "            masks,\n",
    "            gamma,\n",
    "            lam,\n",
    "            ent_coef,\n",
    "            device,\n",
    "        )\n",
    "\n",
    "        # update the actor and critic networks\n",
    "        agent.update_parameters(critic_loss, actor_loss)\n",
    "\n",
    "        # log the losses and entropy\n",
    "        critic_losses.append(critic_loss.detach().cpu().numpy())\n",
    "        actor_losses.append(actor_loss.detach().cpu().numpy())\n",
    "        entropies.append(entropy.detach().mean().cpu().numpy())\n",
    "    return agent, envs_wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result():\n",
    "\n",
    "    \"\"\" plot the results \"\"\"\n",
    "\n",
    "    # %matplotlib inline\n",
    "\n",
    "    rolling_length = 20\n",
    "    fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "    fig.suptitle(\n",
    "        f\"Training plots for {agent.__class__.__name__} in the Swimmer-v4 environment \\n \\\n",
    "                (n_envs={n_envs}, n_steps_per_update={n_steps_per_update}, randomize_domain={randomize_domain})\"\n",
    "    )\n",
    "\n",
    "    # episode return\n",
    "    axs[0][0].set_title(\"Episode Returns\")\n",
    "    episode_returns_moving_average = (\n",
    "        np.convolve(\n",
    "            np.array(envs_wrapper.return_queue).flatten(),\n",
    "            np.ones(rolling_length),\n",
    "            mode=\"valid\",\n",
    "        )\n",
    "        / rolling_length\n",
    "    )\n",
    "    axs[0][0].plot(\n",
    "        np.arange(len(episode_returns_moving_average)) / n_envs,\n",
    "        episode_returns_moving_average,\n",
    "    )\n",
    "    axs[0][0].set_xlabel(\"Number of episodes\")\n",
    "\n",
    "    # entropy\n",
    "    axs[1][0].set_title(\"Entropy\")\n",
    "    entropy_moving_average = (\n",
    "        np.convolve(np.array(entropies), np.ones(rolling_length), mode=\"valid\")\n",
    "        / rolling_length\n",
    "    )\n",
    "    axs[1][0].plot(entropy_moving_average)\n",
    "    axs[1][0].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "    # critic loss\n",
    "    axs[0][1].set_title(\"Critic Loss\")\n",
    "    critic_losses_moving_average = (\n",
    "        np.convolve(\n",
    "            np.array(critic_losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "        )\n",
    "        / rolling_length\n",
    "    )\n",
    "    axs[0][1].plot(critic_losses_moving_average)\n",
    "    axs[0][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "    # actor loss\n",
    "    axs[1][1].set_title(\"Actor Loss\")\n",
    "    actor_losses_moving_average = (\n",
    "        np.convolve(np.array(actor_losses).flatten(), np.ones(rolling_length), mode=\"valid\")\n",
    "        / rolling_length\n",
    "    )\n",
    "    axs[1][1].plot(actor_losses_moving_average)\n",
    "    axs[1][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights():\n",
    "\n",
    "    if not os.path.exists(\"weights\"):\n",
    "        os.mkdir(\"weights\")\n",
    "    \"\"\" save network weights \"\"\"\n",
    "    if save_weights:\n",
    "        torch.save(agent.actor.state_dict(), actor_weights_path)\n",
    "        torch.save(agent.critic.state_dict(), critic_weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights():\n",
    "    \"\"\" load network weights \"\"\"\n",
    "    if load_weights:\n",
    "        agent = A2C(obs_shape, action_shape, device, critic_lr, actor_lr)\n",
    "        agent.actor.load_state_dict(torch.load(actor_weights_path))\n",
    "        agent.critic.load_state_dict(torch.load(critic_weights_path))\n",
    "        agent.actor.eval()\n",
    "        agent.critic.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Blu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\vector\\__init__.py:53: UserWarning: \u001b[33mWARN: `gymnasium.vector.make(...)` is deprecated and will be replaced by `gymnasium.make_vec(...)` in v1.0\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "n_envs = 1\n",
    "n_updates = 10\n",
    "n_steps_per_update = 128\n",
    "envs = gym.vector.make(\"Walker2d-v4\", num_envs=n_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomize_domain = False\n",
    "if randomize_domain:\n",
    "    envs = gym.vector.AsyncVectorEnv(\n",
    "        [\n",
    "            lambda: gym.make(\n",
    "                \"Walker2d-v4\",\n",
    "                forward_reward_weight=1.0,\n",
    "                ctrl_cost_weight=1e-3,\n",
    "                healthy_reward=1.0,\n",
    "                terminate_when_unhealthy=True,\n",
    "                healthy_z_range=(0.8, 2),\n",
    "                healthy_angle_range=(-1, 1),\n",
    "                reset_noise_scale=5e-3,\n",
    "                exclude_current_positions_from_observation=True,\n",
    "                max_episode_steps=600,\n",
    "            )\n",
    "            for i in range(n_envs)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x17 and 32x6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m actor_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m entropies \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m agent, envs_wrapper \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_A2C\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_updates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps_per_update\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_envs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_envs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m plot_result()\n\u001b[0;32m      7\u001b[0m save_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 45\u001b[0m, in \u001b[0;36mtrain_A2C\u001b[1;34m(envs, n_updates, n_steps_per_update, gamma, lam, ent_coef, actor_lr, critic_lr, n_envs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# play n steps in our parallel environments to collect data\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps_per_update):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# select an action A_{t} using S_{t} as input for the agent\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     actions, action_log_probs, state_value_preds, entropy \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstates\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m# perform the action A_{t} in the environment to get S_{t+1} and R_{t+1}\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     states, rewards, terminated, truncated, infos \u001b[38;5;241m=\u001b[39m envs_wrapper\u001b[38;5;241m.\u001b[39mstep(\n\u001b[0;32m     51\u001b[0m         actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     52\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[4], line 44\u001b[0m, in \u001b[0;36mContinuousA2C.select_action\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 44\u001b[0m     state_values, action_means, action_std_devs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m# Create normal distributions and sample actions using reparameterization trick\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     dists \u001b[38;5;241m=\u001b[39m D\u001b[38;5;241m.\u001b[39mNormal(action_means, action_std_devs)\n",
      "Cell \u001b[1;32mIn[4], line 40\u001b[0m, in \u001b[0;36mContinuousA2C.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m state_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(x)  \u001b[38;5;66;03m# shape: [n_envs,]\u001b[39;00m\n\u001b[0;32m     39\u001b[0m action_means \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(x)   \u001b[38;5;66;03m# shape: [n_envs, n_actions]\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m action_std_devs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd_dev\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# shape: [n_envs, n_actions]\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state_values, action_means, action_std_devs\n",
      "File \u001b[1;32mc:\\Users\\Blu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Blu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Blu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Blu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Blu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Blu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x17 and 32x6)"
     ]
    }
   ],
   "source": [
    "critic_losses = []\n",
    "actor_losses = []\n",
    "entropies = []\n",
    "agent, envs_wrapper = train_A2C(envs, n_updates, n_steps_per_update, n_envs=n_envs)\n",
    "plot_result()\n",
    "\n",
    "save_weights = True\n",
    "load_weights = False\n",
    "actor_weights_path = \"weights/actor_weights.h5\"\n",
    "critic_weights_path = \"weights/critic_weights.h5\"\n",
    "\n",
    "save_weights()\n",
    "load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Walker2d-v4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 6\n",
      "Update 0/10\n"
     ]
    }
   ],
   "source": [
    "n_steps_per_update = 128\n",
    "gamma = 0.999\n",
    "lam = 0.95\n",
    "ent_coef = 0.01\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.005\n",
    "obs_shape = envs.single_observation_space.shape[0]\n",
    "action_shape = envs.single_action_space.shape[0]\n",
    "print(obs_shape, action_shape)\n",
    "device = torch.device(\"cpu\")\n",
    "agent = ContinuousA2C(obs_shape, action_shape, device, critic_lr, actor_lr, n_envs)\n",
    "envs_wrapper = gym.wrappers.RecordEpisodeStatistics(envs, deque_size=n_envs * n_updates)\n",
    "sample_phase = 0\n",
    "print(f\"Update {sample_phase}/{n_updates}\")\n",
    "ep_value_preds = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "ep_rewards = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "ep_action_log_probs = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "masks = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "if sample_phase == 0:\n",
    "    states, info = envs_wrapper.reset(seed=42)\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
